{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bVJoeE3hN_6t"
      },
      "source": [
        "Soft Computing Assignment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TbOWh678JUhx"
      },
      "source": [
        "Q1. Explore the Single layer feed fwd neural network with perceptron model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i8NuwORpNLxk"
      },
      "source": [
        "Algorithm:\n",
        "1. Initialize weights w1, w2, ..., wn and bias b with small random values.\n",
        "2. For each training sample (xi, ti):\n",
        "     a. Compute net input:\n",
        "        yin = Σ (wj * xij) + b\n",
        "     b. Apply activation function:\n",
        "        y = 1 if yin >= 0\n",
        "            0 otherwise\n",
        "     c. Update weights and bias if output is wrong:\n",
        "        wj(new) = wj(old) + η * (ti - y) * xij\n",
        "        b(new)  = b(old) + η * (ti - y)\n",
        "3. Repeat for all samples until all outputs are correct or max epochs reached."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CI_z6_ekGFTC",
        "outputId": "1a0a4a85-a900-4027-f975-089ef659c83a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AND Gate:\n",
            "0 0 -> 0\n",
            "0 1 -> 0\n",
            "1 0 -> 0\n",
            "1 1 -> 1\n",
            "\n",
            "OR Gate:\n",
            "0 0 -> 0\n",
            "0 1 -> 1\n",
            "1 0 -> 1\n",
            "1 1 -> 1\n",
            "\n",
            "NAND Gate:\n",
            "0 0 -> 1\n",
            "0 1 -> 1\n",
            "1 0 -> 1\n",
            "1 1 -> 0\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Step activation function\n",
        "def step(x):\n",
        "    return 1 if x >= 0 else 0\n",
        "\n",
        "# Perceptron model\n",
        "def perceptron(x1, x2, w1, w2, b):\n",
        "    return step(w1*x1 + w2*x2 + b)\n",
        "\n",
        "# Test for logic gates\n",
        "inputs = [(0,0), (0,1), (1,0), (1,1)]\n",
        "\n",
        "print(\"AND Gate:\")\n",
        "for x1, x2 in inputs:\n",
        "    print(x1, x2, \"->\", perceptron(x1, x2, 1, 1, -1.5))\n",
        "\n",
        "print(\"\\nOR Gate:\")\n",
        "for x1, x2 in inputs:\n",
        "    print(x1, x2, \"->\", perceptron(x1, x2, 1, 1, -0.5))\n",
        "\n",
        "print(\"\\nNAND Gate:\")\n",
        "for x1, x2 in inputs:\n",
        "    print(x1, x2, \"->\", perceptron(x1, x2, -2, -2, 3))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "impoB0uGJKSK"
      },
      "source": [
        "Q2. Explore the Single layer feed fwd neural network with ADALINE model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3OH-V12No6m"
      },
      "source": [
        "Algorithm:\n",
        "1. Initialize weights w1, w2, ..., wn and bias b.\n",
        "2. Repeat for each epoch until MSE < threshold:\n",
        "     a. For each training pair (xi, ti):\n",
        "          i. Compute net input:\n",
        "             yin = Σ (wj * xij) + b\n",
        "          ii. Compute error:\n",
        "              ei = ti - yin\n",
        "          iii. Update weights:\n",
        "               wj = wj + η * ei * xij\n",
        "               b  = b + η * ei\n",
        "     b. Compute Mean Squared Error:\n",
        "        E = (1 / 2N) * Σ (ei)^2\n",
        "3. Stop when MSE < tolerance or epochs completed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0TqiYNW5HnpM",
        "outputId": "f8a9f51d-9832-4899-bea9-4bbba31359c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 → Weights: [0.1 0.1], Bias: 0.100, MSE: 0.2500\n",
            "Epoch 2 → Weights: [0.16112 0.15922], Bias: 0.132, MSE: 0.1718\n",
            "Epoch 3 → Weights: [0.20258054 0.19808926], Bias: 0.133, MSE: 0.1508\n",
            "Epoch 4 → Weights: [0.23371999 0.22650395], Bias: 0.119, MSE: 0.1411\n",
            "Epoch 5 → Weights: [0.25910386 0.24927607], Bias: 0.099, MSE: 0.1336\n",
            "Epoch 6 → Weights: [0.28099036 0.26876266], Bias: 0.077, MSE: 0.1268\n",
            "Epoch 7 → Weights: [0.3005204  0.28613437], Bias: 0.055, MSE: 0.1206\n",
            "Epoch 8 → Weights: [0.31829198 0.30198762], Bias: 0.034, MSE: 0.1150\n",
            "Epoch 9 → Weights: [0.33463769 0.31664003], Bias: 0.013, MSE: 0.1101\n",
            "Epoch 10 → Weights: [0.34975884 0.3302731 ], Bias: -0.006, MSE: 0.1058\n",
            "Epoch 11 → Weights: [0.3637904  0.34300101], Bias: -0.024, MSE: 0.1020\n",
            "Epoch 12 → Weights: [0.3768326  0.35490402], Bias: -0.040, MSE: 0.0987\n",
            "Epoch 13 → Weights: [0.38896637 0.36604455], Bias: -0.056, MSE: 0.0958\n",
            "Epoch 14 → Weights: [0.40026095 0.37647506], Bias: -0.071, MSE: 0.0933\n",
            "Epoch 15 → Weights: [0.41077779 0.38624192], Bias: -0.084, MSE: 0.0912\n",
            "Epoch 16 → Weights: [0.42057256 0.39538734], Bias: -0.097, MSE: 0.0893\n",
            "Epoch 17 → Weights: [0.42969627 0.40395035], Bias: -0.109, MSE: 0.0876\n",
            "Epoch 18 → Weights: [0.43819596 0.41196733], Bias: -0.120, MSE: 0.0862\n",
            "Epoch 19 → Weights: [0.44611517 0.41947238], Bias: -0.131, MSE: 0.0850\n",
            "Epoch 20 → Weights: [0.45349427 0.42649746], Bias: -0.140, MSE: 0.0839\n",
            "\n",
            "Testing:\n",
            "[0 0] -> Net: -0.140, Output: 0\n",
            "[0 1] -> Net: 0.286, Output: 1\n",
            "[1 0] -> Net: 0.313, Output: 1\n",
            "[1 1] -> Net: 0.740, Output: 1\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Activation (sign) function\n",
        "def activation(y_in):\n",
        "    return 1 if y_in >= 0 else 0\n",
        "\n",
        "# ADALINE training\n",
        "def train_adaline(X, T, lr=0.1, epochs=20):\n",
        "    w = np.zeros(X.shape[1])\n",
        "    b = 0\n",
        "    for epoch in range(epochs):\n",
        "        total_error = 0\n",
        "        for i in range(len(X)):\n",
        "            y_in = np.dot(w, X[i]) + b\n",
        "            error = T[i] - y_in\n",
        "            w += lr * error * X[i]\n",
        "            b += lr * error\n",
        "            total_error += error**2\n",
        "        print(f\"Epoch {epoch+1} → Weights: {w}, Bias: {b:.3f}, MSE: {total_error/len(X):.4f}\")\n",
        "    return w, b\n",
        "\n",
        "# Dataset (AND gate)\n",
        "X = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
        "T = np.array([0,0,0,1])\n",
        "\n",
        "# Train ADALINE\n",
        "w, b = train_adaline(X, T)\n",
        "\n",
        "# Test results\n",
        "print(\"\\nTesting:\")\n",
        "for x in X:\n",
        "    y_in = np.dot(w, x) + b\n",
        "    print(f\"{x} -> Net: {y_in:.3f}, Output: {activation(y_in)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I32fhRqwJdbU"
      },
      "source": [
        "Q3. Explore the multi layer feed fwd neural network with MADALINE model, XOR logic gate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-xL2TpCWNuS3"
      },
      "source": [
        "Algorithm:\n",
        "1. Initialize all weights and biases for each ADALINE in hidden and output layers.\n",
        "2. Forward Pass:\n",
        "     a. For each input x:\n",
        "          i. Compute hidden layer outputs:\n",
        "             hj = f(Σ (wji * xi) + bj)\n",
        "          ii. Compute final output:\n",
        "              y = f(Σ (vj * hj) + c)\n",
        "3. Compute error:\n",
        "     E = t - y\n",
        "4. Weight Adjustment (MADALINE Rule I or II):\n",
        "     Rule I: Use LMS (like ADALINE) to minimize squared error.\n",
        "     Rule II: Flip neuron activations that reduce error most and update their weights.\n",
        "5. Repeat until total error minimized for all patterns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D9DRelWQIZgl",
        "outputId": "d848b2e5-c30d-4f08-acf9-6ffc46b46ab1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "x1 x2 | h1  h2 | y (XOR)\n",
            "------------------------\n",
            " 0  0  |  1   0  |   0\n",
            " 0  1  |  1   1  |   1\n",
            " 1  0  |  1   1  |   1\n",
            " 1  1  |  0   1  |   0\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Step activation function\n",
        "def step(x):\n",
        "    return 1 if x >= 0 else 0\n",
        "\n",
        "# MADALINE XOR implementation\n",
        "def madaline_xor(x1, x2):\n",
        "    # Hidden layer 1 (NAND)\n",
        "    h1_in = -1*x1 + -1*x2 + 1.5\n",
        "    h1 = step(h1_in)\n",
        "\n",
        "    # Hidden layer 2 (OR)\n",
        "    h2_in = 1*x1 + 1*x2 - 0.5\n",
        "    h2 = step(h2_in)\n",
        "\n",
        "    # Output layer (AND)\n",
        "    y_in = 1*h1 + 1*h2 - 1.5\n",
        "    y = step(y_in)\n",
        "\n",
        "    return h1, h2, y\n",
        "\n",
        "# Test all combinations\n",
        "inputs = [(0,0), (0,1), (1,0), (1,1)]\n",
        "\n",
        "print(\"x1 x2 | h1  h2 | y (XOR)\")\n",
        "print(\"-\"*24)\n",
        "for x1, x2 in inputs:\n",
        "    h1, h2, y = madaline_xor(x1, x2)\n",
        "    print(f\" {x1}  {x2}  |  {h1}   {h2}  |   {y}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F95GxTTSKnQ6"
      },
      "source": [
        "Q4. Explore the Multi Layer neural network with backpropagation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHO0K_5ZN2m2"
      },
      "source": [
        "Algorithm:\n",
        "\n",
        "1. Initialization:\n",
        "   - Initialize all weights and biases with small random numbers.\n",
        "   - Set learning rate η.\n",
        "\n",
        "2. Forward Pass:\n",
        "   For each training sample (x, t):\n",
        "     a. Input Layer: Feed the input vector x.\n",
        "     b. Hidden Layer:\n",
        "        zj = Σ (wji * xi) + bj\n",
        "        hj = f(zj)\n",
        "     c. Output Layer:\n",
        "        yk = f(Σ (vkj * hj) + ck)\n",
        "\n",
        "3. Compute Error:\n",
        "     E = (1/2) * Σ (tk - yk)^2\n",
        "\n",
        "4. Backward Pass (Compute Deltas):\n",
        "     a. Output layer delta:\n",
        "        δk = (tk - yk) * f'(yk)\n",
        "     b. Hidden layer delta:\n",
        "        δj = f'(hj) * Σ (δk * vkj)\n",
        "\n",
        "5. Update Weights and Biases:\n",
        "     For output layer:\n",
        "        vkj = vkj + η * δk * hj\n",
        "        ck  = ck + η * δk\n",
        "     For hidden layer:\n",
        "        wji = wji + η * δj * xi\n",
        "        bj  = bj + η * δj\n",
        "\n",
        "6. Repeat for all samples and epochs until total error < threshold."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lqyFsjurKPMH",
        "outputId": "13f78f6d-704b-48bf-ac48-ee07944582db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final outputs after training:\n",
            "[[0.019]\n",
            " [0.984]\n",
            " [0.984]\n",
            " [0.017]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Sigmoid and its derivative\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    return x * (1 - x)\n",
        "\n",
        "# XOR dataset\n",
        "X = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
        "y = np.array([[0],[1],[1],[0]])\n",
        "\n",
        "# Initialize weights and biases randomly\n",
        "np.random.seed(42)\n",
        "w_hidden = np.random.uniform(size=(2,2))  # 2 inputs -> 2 hidden neurons\n",
        "b_hidden = np.random.uniform(size=(1,2))\n",
        "w_output = np.random.uniform(size=(2,1))  # 2 hidden -> 1 output\n",
        "b_output = np.random.uniform(size=(1,1))\n",
        "\n",
        "# Learning rate\n",
        "lr = 0.5\n",
        "\n",
        "# Training\n",
        "for epoch in range(10000):\n",
        "    # Forward pass\n",
        "    hidden_input = np.dot(X, w_hidden) + b_hidden\n",
        "    hidden_output = sigmoid(hidden_input)\n",
        "\n",
        "    final_input = np.dot(hidden_output, w_output) + b_output\n",
        "    final_output = sigmoid(final_input)\n",
        "\n",
        "    # Compute error\n",
        "    error = y - final_output\n",
        "\n",
        "    # Backpropagation\n",
        "    d_output = error * sigmoid_derivative(final_output)\n",
        "    error_hidden = d_output.dot(w_output.T)\n",
        "    d_hidden = error_hidden * sigmoid_derivative(hidden_output)\n",
        "\n",
        "    # Update weights and biases\n",
        "    w_output += hidden_output.T.dot(d_output) * lr\n",
        "    b_output += np.sum(d_output, axis=0, keepdims=True) * lr\n",
        "    w_hidden += X.T.dot(d_hidden) * lr\n",
        "    b_hidden += np.sum(d_hidden, axis=0, keepdims=True) * lr\n",
        "\n",
        "# Test results\n",
        "print(\"Final outputs after training:\")\n",
        "print(final_output.round(3))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
